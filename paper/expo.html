<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link
			rel="stylesheet"
			href="https://unpkg.com/purecss@1.0.1/build/pure-min.css"
			integrity="sha384-oAOxQR6DkCoMliIh8yFnu25d7Eq/PHS21PClpwjOTeU2jRSq11vu66rf90/cZr47"
			crossorigin="anonymous"
		/>
		<link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/grids-responsive-min.css" />
		<link
			rel="stylesheet"
			href="https://use.fontawesome.com/releases/v5.0.12/css/all.css"
			integrity="sha384-G0fIWCsCzJIMAVNQPfjH08cyYaUtMwjJwqiRKxxE/rx96Uroj1BtIQ6MLJuheaO9"
			crossorigin="anonymous"
		/>
		<link rel="preconnect" href="https://fonts.googleapis.com" />
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
		<link
			href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"
			rel="stylesheet"
		/>
		<link rel="stylesheet" href="../global.css" />
		<link rel="icon" href="../favicon.png" />
		
		<link href="../_app/immutable/assets/6.DbAirwOB.css" rel="stylesheet">
		<link href="../_app/immutable/assets/Footer.Dem8Qb1g.css" rel="stylesheet">
		<link href="../_app/immutable/assets/Links.BlD_4Dup.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.BDSvtKH_.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/entry.CzAoIaHB.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/scheduler.DUa3pFyD.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.CTSrl7RS.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index.x8VGGEhm.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.Bv_hjRoc.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/6.B859qmJY.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/js-yaml.mbYHt68G.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Footer.B8LiSrLh.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Links.DGNkMOdY.js">
	</head>
	<body>
		<div>  <div id="body" class="svelte-fk481a"><a href="/" class="home svelte-fk481a" data-svelte-h="svelte-scunpv"><i class="fas fa-home svelte-fk481a" id="home"></i> <h4 id="home-link" class="svelte-fk481a"><span class="color svelte-fk481a">Ángel </span> <span class="color red svelte-fk481a">Alex</span> <span class="color svelte-fk481a">ander </span> <span class="color red svelte-fk481a">Cabrera</span></h4></a> <hr> <h1 class="svelte-fk481a">Regularizing Black-box Models for Improved Interpretability</h1> <div id="info" class="svelte-fk481a"><h3 class="svelte-fk481a"><!-- HTML_TAG_START --><a class='author' href='https://gdplumb.github.io/'>Gregory Plumb</a>, <a class='author' href='https://www.cs.cmu.edu/~mshediva/'>Maruan Al-Shedivat</a>, <a class='me' href='https://cabreraalex.com'>Ángel Alexander Cabrera</a>, <a class='author' href='http://perer.org/'>Adam Perer</a>, <a class='author' href='http://www.cs.cmu.edu/~epxing/'>Eric Xing</a>, <a class='author' href='https://www.cs.cmu.edu/~atalwalk/'>Ameet Talwalkar</a><!-- HTML_TAG_END --></h3></div> <div id="preview" class="svelte-fk481a"><img src="/images/expo.png" class="teaser svelte-fk481a" alt="teaser"> <p class="desc svelte-fk481a">Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, which tend to lack guarantees about the quality of their explanations. We explore a hybridization of these approaches by directly regularizing a black-box model for interpretability at training time - a method we call ExpO. We find that post-hoc explanations of an ExpO-regularized model are consistently more stable and of higher fidelity, which we show theoretically and support empirically. Critically, we also find ExpO leads to explanations that are more actionable, significantly more useful, and more intuitive as supported by a user study.</p></div> <h2 class="sec-title svelte-fk481a" data-svelte-h="svelte-1lwpjo6">Citation</h2> <a href="/paper/expo" class="paper-title svelte-fk481a">Regularizing Black-box Models for Improved Interpretability</a> <h5 class="svelte-fk481a"><!-- HTML_TAG_START --><a class='' href='https://gdplumb.github.io/'>Gregory Plumb</a>, <a class='' href='https://www.cs.cmu.edu/~mshediva/'>Maruan Al-Shedivat</a>, <a class='me' href='https://cabreraalex.com'>Ángel Alexander Cabrera</a>, <a class='' href='http://perer.org/'>Adam Perer</a>, <a class='' href='http://www.cs.cmu.edu/~epxing/'>Eric Xing</a>, <a class='' href='https://www.cs.cmu.edu/~atalwalk/'>Ameet Talwalkar</a><!-- HTML_TAG_END --></h5> <h5 class="svelte-fk481a"><i>Conference on Neural Information Processing Systems (NeurIPS). Vancouver, 2020.</i></h5> <div class="buttons"><a href="https://dl.acm.org/doi/pdf/10.5555/3495724.3496607" rel="external"><button class="entry-link" data-svelte-h="svelte-kcqs98"><i class="fas fa-file-pdf"></i> <p>PDF</p></button></a> <a href="https://dl.acm.org/doi/10.5555/3495724.3496607"><button class="entry-link" data-svelte-h="svelte-q1rqvg"><i class="fas fa-book"></i> <p>BibTex</p></button></a>     <a href="https://github.com/GDPlumb/ExpO"><button class="entry-link" data-svelte-h="svelte-18x59fe"><i class="fab fa-github"></i> <p>Code</p></button></a>  <a href="/paper/expo"><button class="entry-link" data-svelte-h="svelte-ll0vtz"><i class="fas fa-info-circle"></i> <p>Details</p></button></a>  </div> <h2 class="sec-title svelte-fk481a" data-svelte-h="svelte-3ea5wl">BibTex</h2> <div class="code svelte-fk481a"><code class="bibtex">@inproceedings{plumb2020expo, author = {Plumb, Gregory and Al-Shedivat, Maruan and Cabrera, \'{A}ngel Alexander and Perer, Adam and Xing, Eric and Talwalkar, Ameet}, title = {Regularizing Black-Box Models for Improved Interpretability}, year = {2020}, isbn = {9781713829546}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems}, articleno = {883}, numpages = {11}, location = {Vancouver, BC, Canada}, series = {NIPS'20} }</code></div> <div class="footer svelte-1ynsxf6" data-svelte-h="svelte-p4b56a"><p id="copyright">© 2022 Ángel Alexander Cabrera - Made with
		<a href="https://svelte.dev">SvelteKit</a></p> </div> </div> 
			<script type="application/json" data-sveltekit-fetched data-url="/pubs/expo.yml">{"status":200,"statusText":"","headers":{},"body":"title: Regularizing Black-box Models for Improved Interpretability\nid: expo\nteaser: expo.png\nvenue: NeurIPS'20\nvenuelong: Conference on Neural Information Processing Systems (NeurIPS)\nyear: '2020'\nmonth: December\nlocation: Vancouver\nauthors:\n  - name: Gregory Plumb\n    website: 'https://gdplumb.github.io/'\n  - name: Maruan Al-Shedivat\n    website: 'https://www.cs.cmu.edu/~mshediva/'\n  - name: Ángel Alexander Cabrera\n    website: 'https://cabreraalex.com'\n  - name: Adam Perer\n    website: 'http://perer.org/'\n  - name: Eric Xing\n    website: 'http://www.cs.cmu.edu/~epxing/'\n  - name: Ameet Talwalkar\n    website: 'https://www.cs.cmu.edu/~atalwalk/'\nbibtex: >-\n  @inproceedings{plumb2020expo,\n  author = {Plumb, Gregory and Al-Shedivat, Maruan and Cabrera, \\'{A}ngel Alexander and Perer, Adam and Xing, Eric and Talwalkar, Ameet},\n  title = {Regularizing Black-Box Models for Improved Interpretability},\n  year = {2020},\n  isbn = {9781713829546},\n  publisher = {Curran Associates Inc.},\n  address = {Red Hook, NY, USA},\n  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},\n  articleno = {883},\n  numpages = {11},\n  location = {Vancouver, BC, Canada},\n  series = {NIPS'20}\n  }\ncitation: 'https://dl.acm.org/doi/10.5555/3495724.3496607'\nabstract: >-\n  Most of the work on interpretable machine learning has focused on designing\n  either inherently interpretable models, which typically trade-off accuracy\n  for interpretability, or post-hoc explanation systems, which tend to lack\n  guarantees about the quality of their explanations. We explore a\n  hybridization of these approaches by directly regularizing a black-box model\n  for interpretability at training time - a method we call ExpO. We find that\n  post-hoc explanations of an ExpO-regularized model are consistently more\n  stable and of higher fidelity, which we show theoretically and support\n  empirically. Critically, we also find ExpO leads to explanations that are\n  more actionable, significantly more useful, and more intuitive as supported\n  by a user study.\npdf: 'https://dl.acm.org/doi/pdf/10.5555/3495724.3496607'\ncode: 'https://github.com/GDPlumb/ExpO'\n"}</script>
			<script>
				{
					__sveltekit_190nxx = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("../_app/immutable/entry/start.BDSvtKH_.js"),
						import("../_app/immutable/entry/app.CTSrl7RS.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 6],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-L44Q80F4Q9"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag() {
				dataLayer.push(arguments);
			}
			gtag('js', new Date());
			gtag('config', 'G-L44Q80F4Q9');
		</script>
	</body>
</html>
