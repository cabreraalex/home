<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="../favicon.png" />
		<link rel="stylesheet" href="../global.css" />
		<link
			rel="stylesheet"
			href="https://unpkg.com/purecss@1.0.1/build/pure-min.css"
			integrity="sha384-oAOxQR6DkCoMliIh8yFnu25d7Eq/PHS21PClpwjOTeU2jRSq11vu66rf90/cZr47"
			crossorigin="anonymous"
		/>
		<link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/grids-responsive-min.css" />

		<link
			rel="stylesheet"
			href="https://use.fontawesome.com/releases/v5.0.12/css/all.css"
			integrity="sha384-G0fIWCsCzJIMAVNQPfjH08cyYaUtMwjJwqiRKxxE/rx96Uroj1BtIQ6MLJuheaO9"
			crossorigin="anonymous"
		/>
		<link
			href="https://fonts.googleapis.com/css?family=Open+Sans:400|Roboto:900,400"
			rel="stylesheet"
		/>
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
	<link rel="stylesheet" href="/_app/assets/pages/paper/_slug_.svelte-e53b8673.css">
	<link rel="stylesheet" href="/_app/assets/Footer-6c39282c.css">
	<link rel="modulepreload" href="/_app/start-0e403804.js">
	<link rel="modulepreload" href="/_app/chunks/vendor-e42abbb2.js">
	<link rel="modulepreload" href="/_app/layout.svelte-c11e8583.js">
	<link rel="modulepreload" href="/_app/pages/paper/_slug_.svelte-bac8f889.js">
	<link rel="modulepreload" href="/_app/chunks/Footer-b9186261.js">
	<link rel="modulepreload" href="/_app/chunks/Links-b89499bd.js">
			<script type="module">
				import { start } from "/_app/start-0e403804.js";
				start({
					target: document.querySelector("#svelte"),
					paths: {"base":"","assets":""},
					session: {},
					route: true,
					spa: false,
					trailing_slash: "never",
					hydrate: {
						status: 200,
						error: null,
						nodes: [
							import("/_app/layout.svelte-c11e8583.js"),
						import("/_app/pages/paper/_slug_.svelte-bac8f889.js")
						],
						url: new URL("http://prerender/paper/expo"),
						params: {slug:"expo"}
					}
				});
			</script>
	</head>
	<body>
		<div id="svelte">


<div id="body" class="svelte-fb5nlw"><a href="/" class="home svelte-fb5nlw"><i class="fas fa-home svelte-fb5nlw" id="home"></i>
		<h4 id="home-link" class="svelte-fb5nlw"><span class="color svelte-fb5nlw">Ángel </span>
			<span class="color red svelte-fb5nlw">Alex</span>
			<span class="color svelte-fb5nlw">ander </span>
			<span class="color red svelte-fb5nlw">Cabrera</span></h4></a>
	<hr>
	<h1 class="svelte-fb5nlw">Regularizing Black-box Models for Improved Interpretability</h1>
	<div id="info" class="svelte-fb5nlw"><h3 class="svelte-fb5nlw"><!-- HTML_TAG_START --><a class='author' href='https://gdplumb.github.io/'>Gregory Plumb</a>, <a class='author' href='https://www.cs.cmu.edu/~mshediva/'>Maruan Al-Shedivat</a>, <a class='me' href='http://cabreraalex.com'>Ángel Alexander Cabrera</a>, <a class='author' href='http://perer.org/'>Adam Perer</a>, <a class='author' href='http://www.cs.cmu.edu/~epxing/'>Eric Xing</a>, <a class='author' href='https://www.cs.cmu.edu/~atalwalk/'>Ameet Talwalkar</a><!-- HTML_TAG_END --></h3></div>
	<div class="flex pure-g svelte-fb5nlw"><div class="pure-u-1 pure-u-md-1-2"><img src="/images/expo.png" class="teaser svelte-fb5nlw" alt="teaser"></div>
		<div class="pure-u-1 pure-u-md-1-2"><p class="desc svelte-fb5nlw">We introduce a new regularization method for training deep learning models that improves the stability and fidelity of post-hoc explanantion methods like LIME. Through a user study we show that the regularized model empirically improves the quality of explainations.</p></div></div>

	<h2 class="sec-title svelte-fb5nlw">Abstract</h2>
	<p class="svelte-fb5nlw">Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, which tend to lack guarantees about the quality of their explanations. We explore a hybridization of these approaches by directly regularizing a black-box model for interpretability at training time - a method we call ExpO. We find that post-hoc explanations of an ExpO-regularized model are consistently more stable and of higher fidelity, which we show theoretically and support empirically. Critically, we also find ExpO leads to explanations that are more actionable, significantly more useful, and more intuitive as supported by a user study.</p>

	<h2 class="sec-title svelte-fb5nlw">Citation</h2>
	<a href="http://cabreraalex.com/paper/expo" class="paper-title"><h4 class="svelte-fb5nlw">Regularizing Black-box Models for Improved Interpretability</h4></a>

	<h5 class="svelte-fb5nlw"><!-- HTML_TAG_START --><a class='' href='https://gdplumb.github.io/'>Gregory Plumb</a>, <a class='' href='https://www.cs.cmu.edu/~mshediva/'>Maruan Al-Shedivat</a>, <a class='me' href='http://cabreraalex.com'>Ángel Alexander Cabrera</a>, <a class='' href='http://perer.org/'>Adam Perer</a>, <a class='' href='http://www.cs.cmu.edu/~epxing/'>Eric Xing</a>, <a class='' href='https://www.cs.cmu.edu/~atalwalk/'>Ameet Talwalkar</a><!-- HTML_TAG_END --></h5>

	<h5 class="svelte-fb5nlw"><i>Conference on Neural Information Processing Systems (NeurIPS). Vancouver, 2020.</i></h5>

	<div class="buttons"><a href="https://arxiv.org/pdf/1902.06787.pdf"><button class="entry-link"><i class="fas fa-file-pdf"></i>
				<p>PDF</p></button></a>
	<a href="https://proceedings.neurips.cc/paper/10607-/bibtex"><button class="entry-link"><i class="fas fa-book"></i>
				<p>BibTex</p></button></a>
	
	
	
	
	<a href="https://github.com/GDPlumb/ExpO"><button class="entry-link"><i class="fab fa-github"></i>
				<p>Code</p></button></a>
	
	<a href="http://cabreraalex.com/paper/expo"><button class="entry-link"><i class="fas fa-globe"></i>
			<p>Website</p></button></a></div>
	<h2 class="sec-title svelte-fb5nlw">BibTex</h2>
	<div class="code svelte-fb5nlw"><code class="bibtex">@inproceedings{plumb2020expo,author = {Plumb, Gregory and Al-Shedivat, Maruan and Cabrera, &#39;{A}ngel Alexander and Perer, Adam and Xing, Eric and Talwalkar, Ameet},booktitle = {Advances in Neural Information Processing Systems},pages = {10526--10536},publisher = {Curran Associates, Inc.},title = {Regularizing Black-box Models for Improved Interpretability},url = {https://proceedings.neurips.cc/paper/2020/file/770f8e448d07586afbf77bb59f698587-Paper.pdf},volume = {33},year = {2020}}</code></div>
	<div class="footer svelte-1ynsxf6"><p id="copyright">© 2022 Ángel Alexander Cabrera - Made with
		<a href="https://svelte.dev">SvelteKit</a></p>
</div>
</div>

<script type="application/json" data-type="svelte-data" data-url="/pubs/expo.yml">{"status":200,"statusText":"","headers":{"content-type":"text/yaml"},"body":"title: Regularizing Black-box Models for Improved Interpretability\ndesc: \u003E-\n  We introduce a new regularization method for training deep learning models\n  that improves the stability and fidelity of post-hoc explanantion methods\n  like LIME. Through a user study we show that the regularized model\n  empirically improves the quality of explainations.\nid: expo\nteaser: expo.png\nvenue: NeurIPS'20\nvenuelong: Conference on Neural Information Processing Systems (NeurIPS)\nyear: '2020'\nmonth: December\nlocation: Vancouver\nauthors:\n  - name: Gregory Plumb\n    website: 'https:\u002F\u002Fgdplumb.github.io\u002F'\n  - name: Maruan Al-Shedivat\n    website: 'https:\u002F\u002Fwww.cs.cmu.edu\u002F~mshediva\u002F'\n  - name: Ángel Alexander Cabrera\n    website: 'http:\u002F\u002Fcabreraalex.com'\n  - name: Adam Perer\n    website: 'http:\u002F\u002Fperer.org\u002F'\n  - name: Eric Xing\n    website: 'http:\u002F\u002Fwww.cs.cmu.edu\u002F~epxing\u002F'\n  - name: Ameet Talwalkar\n    website: 'https:\u002F\u002Fwww.cs.cmu.edu\u002F~atalwalk\u002F'\nbibtex: \u003E-\n  @inproceedings{plumb2020expo,author = {Plumb, Gregory and Al-Shedivat,\n  Maruan and Cabrera, '{A}ngel Alexander and Perer, Adam and Xing, Eric and\n  Talwalkar, Ameet},booktitle = {Advances in Neural Information Processing\n  Systems},pages = {10526--10536},publisher = {Curran Associates, Inc.},title\n  = {Regularizing Black-box Models for Improved Interpretability},url =\n  {https:\u002F\u002Fproceedings.neurips.cc\u002Fpaper\u002F2020\u002Ffile\u002F770f8e448d07586afbf77bb59f698587-Paper.pdf},volume\n  = {33},year = {2020}}\ncitation: 'https:\u002F\u002Fproceedings.neurips.cc\u002Fpaper\u002F10607-\u002Fbibtex'\nabstract: \u003E-\n  Most of the work on interpretable machine learning has focused on designing\n  either inherently interpretable models, which typically trade-off accuracy\n  for interpretability, or post-hoc explanation systems, which tend to lack\n  guarantees about the quality of their explanations. We explore a\n  hybridization of these approaches by directly regularizing a black-box model\n  for interpretability at training time - a method we call ExpO. We find that\n  post-hoc explanations of an ExpO-regularized model are consistently more\n  stable and of higher fidelity, which we show theoretically and support\n  empirically. Critically, we also find ExpO leads to explanations that are\n  more actionable, significantly more useful, and more intuitive as supported\n  by a user study.\npdf: 'https:\u002F\u002Farxiv.org\u002Fpdf\u002F1902.06787.pdf'\ncode: 'https:\u002F\u002Fgithub.com\u002FGDPlumb\u002FExpO'\n"}</script></div>
	</body>
</html>
