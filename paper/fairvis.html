<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link
			rel="stylesheet"
			href="https://unpkg.com/purecss@1.0.1/build/pure-min.css"
			integrity="sha384-oAOxQR6DkCoMliIh8yFnu25d7Eq/PHS21PClpwjOTeU2jRSq11vu66rf90/cZr47"
			crossorigin="anonymous"
		/>
		<link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/grids-responsive-min.css" />
		<link
			rel="stylesheet"
			href="https://use.fontawesome.com/releases/v5.0.12/css/all.css"
			integrity="sha384-G0fIWCsCzJIMAVNQPfjH08cyYaUtMwjJwqiRKxxE/rx96Uroj1BtIQ6MLJuheaO9"
			crossorigin="anonymous"
		/>
		<link rel="preconnect" href="https://fonts.googleapis.com" />
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
		<link
			href="https://fonts.googleapis.com/css2?family=Barlow:wght@300;400;600&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css?family=Open+Sans:400|Roboto:900,400"
			rel="stylesheet"
		/>
		<link rel="stylesheet" href="../global.css" />
		<link rel="icon" href="../favicon.png" />
		<meta http-equiv="content-security-policy" content="">
		<link href="../_app/immutable/assets/_page-ae5eb0fe.css" rel="stylesheet">
		<link href="../_app/immutable/assets/Footer-6b856700.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/start-7d40a187.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index-0696beeb.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/singletons-77fbefdd.js">
		<link rel="modulepreload" href="../_app/immutable/components/layout.svelte-09dc7409.js">
		<link rel="modulepreload" href="../_app/immutable/modules/pages/_layout.js-7b9cbfbc.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_layout-8d2a742b.js">
		<link rel="modulepreload" href="../_app/immutable/components/pages/paper/_slug_/_page.svelte-715a0f19.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Footer-d186a1f9.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Links-7ae1522f.js">
		<link rel="modulepreload" href="../_app/immutable/modules/pages/paper/_slug_/_page.js-491846b6.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/js-yaml-f8efc9bc.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_page-f2f4d657.js">
	</head>
	<body>
		<div>


<div id="body" class="svelte-s1lhzh"><a href="/" class="home svelte-s1lhzh"><i class="fas fa-home svelte-s1lhzh" id="home"></i>
		<h4 id="home-link" class="svelte-s1lhzh"><span class="color svelte-s1lhzh">Ángel </span>
			<span class="color red svelte-s1lhzh">Alex</span>
			<span class="color svelte-s1lhzh">ander </span>
			<span class="color red svelte-s1lhzh">Cabrera</span></h4></a>
	<hr>
	<h1 class="svelte-s1lhzh">FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning</h1>
	<div id="info" class="svelte-s1lhzh"><h3 class="svelte-s1lhzh"><!-- HTML_TAG_START --><a class='me' href='https://cabreraalex.com'>Ángel Alexander Cabrera</a>, <a class='author' href='http://willepperson.com'>Will Epperson</a>, <a class='author' href='https://fredhohman.com'>Fred Hohman</a>, <a class='author' href='https://minsuk.com'>Minsuk Kahng</a>, <a class='author' href='http://jamiemorgenstern.com'>Jamie Morgenstern</a>, <a class='author' href='https://poloclub.github.io/polochau/'>Duen Horng (Polo) Chau</a><!-- HTML_TAG_END --></h3></div>
	<div id="preview" class="svelte-s1lhzh"><img src="/images/fairvis.png" class="teaser svelte-s1lhzh" alt="teaser">
		<p class="desc svelte-s1lhzh">The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis' coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p></div>

	

	<h2 class="sec-title svelte-s1lhzh">Citation</h2>
	<a href="/paper/fairvis" class="paper-title"><h4 class="svelte-s1lhzh">FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning</h4></a>

	<h5 class="svelte-s1lhzh"><!-- HTML_TAG_START --><a class='me' href='https://cabreraalex.com'>Ángel Alexander Cabrera</a>, <a class='' href='http://willepperson.com'>Will Epperson</a>, <a class='' href='https://fredhohman.com'>Fred Hohman</a>, <a class='' href='https://minsuk.com'>Minsuk Kahng</a>, <a class='' href='http://jamiemorgenstern.com'>Jamie Morgenstern</a>, <a class='' href='https://poloclub.github.io/polochau/'>Duen Horng (Polo) Chau</a><!-- HTML_TAG_END --></h5>

	<h5 class="svelte-s1lhzh"><i>IEEE Conference on Visual Analytics Science and Technology (VAST). Vancouver, Canada, 2019.</i></h5>

	<div class="buttons"><a href="https://arxiv.org/abs/1904.05419" rel="external"><button class="entry-link"><i class="fas fa-file-pdf"></i>
				<p>PDF</p></button></a>
	<a href="https://ieeexplore.ieee.org/document/8986948"><button class="entry-link"><i class="fas fa-book"></i>
				<p>BibTex</p></button></a>
	<a href="https://poloclub.github.io/FairVis/"><button class="entry-link"><i class="fas fa-globe"></i>
				<p>Website</p></button></a>
	<a href="https://medium.com/@cabreraalex/fairvis-discovering-bias-in-machine-learning-using-visual-analytics-acbd362a3e2f"><button class="entry-link"><i class="fab fa-medium"></i>
				<p>Blog</p></button></a>
	
	<a href="https://vimeo.com/showcase/6524122/video/368702211"><button class="entry-link"><i class="fab fa-youtube"></i>
				<p>Video</p></button></a>
	<a href="https://github.com/poloclub/FairVis"><button class="entry-link"><i class="fab fa-github"></i>
				<p>Code</p></button></a>
	
	<a href="/paper/fairvis"><button class="entry-link"><i class="fas fa-info-circle"></i>
			<p>Details</p></button></a></div>
	<h2 class="sec-title svelte-s1lhzh">BibTex</h2>
		<div class="code svelte-s1lhzh"><code class="bibtex">@INPROCEEDINGS{cabrera2019fairvis, author={Á. A. {Cabrera} and W. {Epperson} and F. {Hohman} and M. {Kahng} and J. {Morgenstern} and D. H. {Chau}}, booktitle={2019 IEEE Conference on Visual Analytics Science and Technology (VAST)}, title={FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning}, year={2019}, volume={}, number={}, pages={46-56},doi={10.1109/VAST47406.2019.8986948}}</code></div>
	<div class="footer svelte-1ynsxf6"><p id="copyright">© 2022 Ángel Alexander Cabrera - Made with
		<a href="https://svelte.dev">SvelteKit</a></p>
</div>
</div>


		<script type="module" data-sveltekit-hydrate="nynwal">
			import { start } from "../_app/immutable/start-7d40a187.js";

			start({
				env: {},
				hydrate: {
					status: 200,
					error: null,
					node_ids: [0, 7],
					params: {slug:"fairvis"},
					routeId: "/paper/[slug]",
					data: (function(a){return [a,a]}(null)),
					form: null
				},
				paths: {"base":"","assets":""},
				target: document.querySelector('[data-sveltekit-hydrate="nynwal"]').parentNode,
				trailing_slash: "never"
			});
		</script>
	<script type="application/json" data-sveltekit-fetched data-url="/pubs/fairvis.yml">{"status":200,"statusText":"","headers":{},"body":"title: >-\n  FairVis: Visual Analytics for Discovering Intersectional Bias in Machine\n  Learning\ndesc: >-\n  FairVis is a visual analytics system that enables data scientists to find\n  potential biases in their machine learning models. It allows users to split\n  their data into subgroups of different features to see how vulnerable groups\n  are performing for various fairness metrics. Additionally, it suggests\n  groups that may be underperforming and can find similar groups.\nid: fairvis\nteaser: fairvis.png\nvenue: VIS'19\nvenuelong: IEEE Conference on Visual Analytics Science and Technology (VAST)\nyear: '2019'\nmonth: October\nlocation: 'Vancouver, Canada'\nauthors:\n  - name: Ángel Alexander Cabrera\n    website: 'https://cabreraalex.com'\n  - name: Will Epperson\n    website: 'http://willepperson.com'\n  - name: Fred Hohman\n    website: 'https://fredhohman.com'\n  - name: Minsuk Kahng\n    website: 'https://minsuk.com'\n  - name: Jamie Morgenstern\n    website: 'http://jamiemorgenstern.com'\n  - name: Duen Horng (Polo) Chau\n    website: 'https://poloclub.github.io/polochau/'\nbibtex: >-\n  @INPROCEEDINGS{cabrera2019fairvis, author={Á. A. {Cabrera} and W. {Epperson}\n  and F. {Hohman} and M. {Kahng} and J. {Morgenstern} and D. H. {Chau}},\n  booktitle={2019 IEEE Conference on Visual Analytics Science and Technology\n  (VAST)}, title={FAIRVIS: Visual Analytics for Discovering Intersectional\n  Bias in Machine Learning}, year={2019}, volume={}, number={},\n  pages={46-56},doi={10.1109/VAST47406.2019.8986948}}\nabstract: >-\n  The growing capability and accessibility of machine learning has led to its\n  application to many real-world domains and data about people. Despite the\n  benefits algorithmic systems may bring, models can reflect, inject, or\n  exacerbate implicit and explicit societal biases into their outputs,\n  disadvantaging certain demographic subgroups. Discovering which biases a\n  machine learning model has introduced is a great challenge, due to the\n  numerous definitions of fairness and the large number of potentially\n  impacted subgroups. We present FairVis, a mixed-initiative visual analytics\n  system that integrates a novel subgroup discovery technique for users to\n  audit the fairness of machine learning models. Through FairVis, users can\n  apply domain knowledge to generate and investigate known subgroups, and\n  explore suggested and similar subgroups. FairVis' coordinated views enable\n  users to explore a high-level overview of subgroup performance and\n  subsequently drill down into detailed investigation of specific subgroups.\n  We show how FairVis helps to discover biases in two real datasets used in\n  predicting income and recidivism. As a visual analytics system devoted to\n  discovering bias in machine learning, FairVis demonstrates how interactive\n  visualization may help data scientists and the general public understand and\n  create more equitable algorithmic systems.\nweb: 'https://poloclub.github.io/FairVis/'\ncode: 'https://github.com/poloclub/FairVis'\nblog: >-\n  https://medium.com/@cabreraalex/fairvis-discovering-bias-in-machine-learning-using-visual-analytics-acbd362a3e2f\npdf: 'https://arxiv.org/abs/1904.05419'\nvideo: 'https://vimeo.com/showcase/6524122/video/368702211'\ncitation: 'https://ieeexplore.ieee.org/document/8986948'\n"}</script></div>
		<!-- Panelbear -->
		<script async src="https://cdn.panelbear.com/analytics.js?site=F1SD7mTQIuD"></script>
		<script>
			window.panelbear =
				window.panelbear ||
				function () {
					(window.panelbear.q = window.panelbear.q || []).push(arguments);
				};
			panelbear('config', { site: 'F1SD7mTQIuD' });
		</script>
	</body>
</html>
