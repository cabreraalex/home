<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link
			rel="stylesheet"
			href="https://unpkg.com/purecss@1.0.1/build/pure-min.css"
			integrity="sha384-oAOxQR6DkCoMliIh8yFnu25d7Eq/PHS21PClpwjOTeU2jRSq11vu66rf90/cZr47"
			crossorigin="anonymous"
		/>
		<link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/grids-responsive-min.css" />
		<link
			rel="stylesheet"
			href="https://use.fontawesome.com/releases/v5.0.12/css/all.css"
			integrity="sha384-G0fIWCsCzJIMAVNQPfjH08cyYaUtMwjJwqiRKxxE/rx96Uroj1BtIQ6MLJuheaO9"
			crossorigin="anonymous"
		/>
		<link rel="preconnect" href="https://fonts.googleapis.com" />
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
		<link
			href="https://fonts.googleapis.com/css2?family=Barlow:wght@300;400;600&display=swap"
			rel="stylesheet"
		/>
		<link
			href="https://fonts.googleapis.com/css?family=Open+Sans:400|Roboto:900,400"
			rel="stylesheet"
		/>
		<link rel="stylesheet" href="../global.css" />
		<link rel="icon" href="../favicon.png" />
		<meta http-equiv="content-security-policy" content="">
		<link href="../_app/immutable/assets/_page-ae5eb0fe.css" rel="stylesheet">
		<link href="../_app/immutable/assets/Footer-6b856700.css" rel="stylesheet">
		<link href="../_app/immutable/assets/Links-141474e6.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/start-1ae44734.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index-d9827912.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/singletons-3a7d68ef.js">
		<link rel="modulepreload" href="../_app/immutable/components/layout.svelte-22ef87ee.js">
		<link rel="modulepreload" href="../_app/immutable/modules/pages/_layout.js-9cbb603b.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_layout-da46b06b.js">
		<link rel="modulepreload" href="../_app/immutable/components/pages/paper/_slug_/_page.svelte-42b64e82.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Footer-c4137f71.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Links-426cfab2.js">
		<link rel="modulepreload" href="../_app/immutable/modules/pages/paper/_slug_/_page.js-5fc89f97.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/js-yaml-38530ef5.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/_page-1822699d.js">
	</head>
	<body>
		<div>


<div id="body" class="svelte-s1lhzh"><a href="/" class="home svelte-s1lhzh"><i class="fas fa-home svelte-s1lhzh" id="home"></i>
		<h4 id="home-link" class="svelte-s1lhzh"><span class="color svelte-s1lhzh">Ángel </span>
			<span class="color red svelte-s1lhzh">Alex</span>
			<span class="color svelte-s1lhzh">ander </span>
			<span class="color red svelte-s1lhzh">Cabrera</span></h4></a>
	<hr>
	<h1 class="svelte-s1lhzh">Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation</h1>
	<div id="info" class="svelte-s1lhzh"><h3 class="svelte-s1lhzh"><!-- HTML_TAG_START --><a class='me' href='https://cabreraalex.com'>Ángel Alexander Cabrera</a>, <a class='author' href='https://minsuk.com'>Minsuk Kahng</a>, <a class='author' href='https://fredhohman.com'>Fred Hohman</a>, <a class='author' href='http://jamiemorgenstern.com'>Jamie Morgenstern</a>, <a class='author' href='https://poloclub.github.io/polochau/'>Duen Horng (Polo) Chau</a><!-- HTML_TAG_END --></h3></div>
	<div id="preview" class="svelte-s1lhzh"><img src="/images/iclr.png" class="teaser svelte-s1lhzh" alt="teaser">
		<p class="desc svelte-s1lhzh">As machine learning is applied to data about people, it is crucial to understand how learned models treat different demographic groups. Many factors, including what training data and class of models are used, can encode biased behavior into learned outcomes. These biases are often small when considering a single feature (e.g., sex or race) in isolation, but appear more blatantly at the intersection of multiple features. We present our ongoing work of designing automatic techniques and interactive tools to help users discover subgroups of data instances on which a model underperforms. Using a bottom-up clustering technique for subgroup generation, users can quickly find areas of a dataset in which their models are encoding bias. Our work presents some of the first user-focused, interactive methods for discovering bias in machine learning models.</p></div>

	

	<h2 class="sec-title svelte-s1lhzh">Citation</h2>
	<a href="/paper/subgroup-gen" class="paper-title"><h4 class="svelte-s1lhzh">Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation</h4></a>

	<h5 class="svelte-s1lhzh"><!-- HTML_TAG_START --><a class='me' href='https://cabreraalex.com'>Ángel Alexander Cabrera</a>, <a class='' href='https://minsuk.com'>Minsuk Kahng</a>, <a class='' href='https://fredhohman.com'>Fred Hohman</a>, <a class='' href='http://jamiemorgenstern.com'>Jamie Morgenstern</a>, <a class='' href='https://poloclub.github.io/polochau/'>Duen Horng (Polo) Chau</a><!-- HTML_TAG_END --></h5>

	<h5 class="svelte-s1lhzh"><i>ICLR - Debugging Machine Learning Models Workshop (Debug ML). New Orleans, Louisiana, USA, 2019.</i></h5>

	<div class="buttons"><a href="https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_3.pdf" rel="external"><button class="entry-link"><i class="fas fa-file-pdf"></i>
				<p>PDF</p></button></a>
	
	
	
	<a href="https://debug-ml-iclr2019.github.io/"><button class="entry-link"><i class="fas fa-globe"></i>
				<p>Workshop</p></button></a>
	
	
	
	<a href="/paper/subgroup-gen"><button class="entry-link"><i class="fas fa-info-circle"></i>
			<p>Details</p></button></a>
	
</div>
	<h2 class="sec-title svelte-s1lhzh">BibTex</h2>
		<div class="code svelte-s1lhzh"><code class="bibtex">@article{cabrera2019discovery, title={Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation}, author={Cabrera, Ángel Alexander and Kahng, Minsuk and Hohman, Fred and Morgenstern, Jamie and Chau, Duen Horng}, journal={Debugging Machine Learning Models Workshop (Debug ML) at ICLR}, year={2019}}</code></div>
	<div class="footer svelte-1ynsxf6"><p id="copyright">© 2022 Ángel Alexander Cabrera - Made with
		<a href="https://svelte.dev">SvelteKit</a></p>
</div>
</div>


		<script type="module" data-sveltekit-hydrate="fe0mko">
			import { start } from "../_app/immutable/start-1ae44734.js";

			start({
				env: {},
				hydrate: {
					status: 200,
					error: null,
					node_ids: [0, 7],
					params: {slug:"subgroup-gen"},
					routeId: "/paper/[slug]",
					data: (function(a){return [a,a]}(null)),
					form: null
				},
				paths: {"base":"","assets":""},
				target: document.querySelector('[data-sveltekit-hydrate="fe0mko"]').parentNode,
				trailing_slash: "never"
			});
		</script>
	<script type="application/json" data-sveltekit-fetched data-url="/pubs/subgroup-gen.yml">{"status":200,"statusText":"","headers":{},"body":"title: >-\n  Discovery of Intersectional Bias in Machine Learning Using Automatic\n  Subgroup Generation\ndesc: >-\n  We introduce a method for automatically generating subgroups of instances\n  that a model may be biased against. The instances are first clustered and\n  then described by their dominating features. By ranking and sorting the\n  groups by their performance metrics (F1, accuracy, etc. ) users can spot\n  groups that are underperforming.\nid: subgroup-gen\nteaser: iclr.png\nvenue: \"Workshop, ICLR'19\"\nvenuelong: ICLR - Debugging Machine Learning Models Workshop (Debug ML)\nyear: '2019'\nmonth: May\nlocation: 'New Orleans, Louisiana, USA'\nauthors:\n  - name: Ángel Alexander Cabrera\n    website: 'https://cabreraalex.com'\n  - name: Minsuk Kahng\n    website: 'https://minsuk.com'\n  - name: Fred Hohman\n    website: 'https://fredhohman.com'\n  - name: Jamie Morgenstern\n    website: 'http://jamiemorgenstern.com'\n  - name: Duen Horng (Polo) Chau\n    website: 'https://poloclub.github.io/polochau/'\nbibtex: >-\n  @article{cabrera2019discovery, title={Discovery of Intersectional Bias in\n  Machine Learning Using Automatic Subgroup Generation}, author={Cabrera,\n  Ángel Alexander and Kahng, Minsuk and Hohman, Fred and Morgenstern, Jamie\n  and Chau, Duen Horng}, journal={Debugging Machine Learning Models Workshop\n  (Debug ML) at ICLR}, year={2019}}\nabstract: >-\n  As machine learning is applied to data about people, it is crucial to\n  understand how learned models treat different demographic groups. Many\n  factors, including what training data and class of models are used, can\n  encode biased behavior into learned outcomes. These biases are often small\n  when considering a single feature (e.g., sex or race) in isolation, but\n  appear more blatantly at the intersection of multiple features. We present\n  our ongoing work of designing automatic techniques and interactive tools to\n  help users discover subgroups of data instances on which a model\n  underperforms. Using a bottom-up clustering technique for subgroup\n  generation, users can quickly find areas of a dataset in which their models\n  are encoding bias. Our work presents some of the first user-focused,\n  interactive methods for discovering bias in machine learning models.\npdf: 'https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_3.pdf'\nworkshop: 'https://debug-ml-iclr2019.github.io/'\n"}</script></div>
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-L44Q80F4Q9"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-L44Q80F4Q9');
		</script>
	</body>
</html>
