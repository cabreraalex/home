title: >-
  Discovery of Intersectional Bias in Machine Learning Using Automatic
  Subgroup Generation
desc: >-
  We introduce a method for automatically generating subgroups of instances
  that a model may be biased against. The instances are first clustered and
  then described by their dominating features. By ranking and sorting the
  groups by their performance metrics (F1, accuracy, etc. ) users can spot
  groups that are underperforming.
id: subgroup-gen
teaser: iclr.png
venue: "Workshop, ICLR'19"
venuelong: Debugging Machine Learning Models Workshop (Debug ML) at ICLR
year: '2019'
month: May
location: 'New Orleans, Louisiana, USA'
authors:
  - name: Ángel Alexander Cabrera
    website: 'http://cabreraalex.com'
  - name: Minsuk Kahng
    website: 'https://minsuk.com'
  - name: Fred Hohman
    website: 'https://fredhohman.com'
  - name: Jamie Morgenstern
    website: 'http://jamiemorgenstern.com'
  - name: Duen Horng (Polo) Chau
    website: 'https://poloclub.github.io/polochau/'
bibtex: >-
  @article{cabrera2019discovery, title={Discovery of Intersectional Bias in
  Machine Learning Using Automatic Subgroup Generation}, author={Cabrera,
  Ángel Alexander and Kahng, Minsuk and Hohman, Fred and Morgenstern, Jamie
  and Chau, Duen Horng}, journal={Debugging Machine Learning Models Workshop
  (Debug ML) at ICLR}, year={2019}}
abstract: >-
  As machine learning is applied to data about people, it is crucial to
  understand how learned models treat different demographic groups. Many
  factors, including what training data and class of models are used, can
  encode biased behavior into learned outcomes. These biases are often small
  when considering a single feature (e.g., sex or race) in isolation, but
  appear more blatantly at the intersection of multiple features. We present
  our ongoing work of designing automatic techniques and interactive tools to
  help users discover subgroups of data instances on which a model
  underperforms. Using a bottom-up clustering technique for subgroup
  generation, users can quickly find areas of a dataset in which their models
  are encoding bias. Our work presents some of the first user-focused,
  interactive methods for discovering bias in machine learning models.
pdf: 'https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_3.pdf'
workshop: 'https://debug-ml-iclr2019.github.io/'
